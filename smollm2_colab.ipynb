{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SmolLM2-135M Fine-Tuning on Colab\n",
        "\n",
        "This notebook walks through adapting the official `config_smollm2_135M.yaml` recipe to Google Colab. It installs dependencies, downloads the base model and tokenizer, prepares the `input.txt` corpus, trains for 5,000 steps with periodic text generation every 500 steps, checkpoints the run, and then resumes for 50 additional steps.\n",
        "\n",
        "> **Tip:** Set your Colab runtime to GPU (A100/TPU preferred, but T4 will also work with the settings below).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "!nvidia-smi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install --upgrade --force-reinstall -q numpy==1.26.4 transformers==4.44.2 accelerate==0.33.0 datasets==3.0.1 evaluate==0.4.3 sentencepiece==0.2.0 omegaconf==2.3.0 bitsandbytes==0.43.1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "print(\"NumPy version:\", np.__version__)\n",
        "assert np.__version__ == \"1.26.4\", \"Please restart the runtime and rerun the install cell before proceeding.\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> If this cell re-runs after you already imported libraries, restart the Colab runtime (`Runtime â†’ Restart runtime`) so the pinned NumPy wheel loads before importing `datasets` or `transformers`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import json\n",
        "import textwrap\n",
        "\n",
        "import requests\n",
        "import yaml\n",
        "\n",
        "CONFIG_URL = \"https://raw.githubusercontent.com/huggingface/smollm/main/text/pretraining/smollm2/config_smollm2_135M.yaml\"\n",
        "CONFIG_PATH = Path(\"config_smollm2_135M.yaml\")\n",
        "\n",
        "if not CONFIG_PATH.exists():\n",
        "    resp = requests.get(CONFIG_URL, timeout=30)\n",
        "    resp.raise_for_status()\n",
        "    CONFIG_PATH.write_text(resp.text)\n",
        "\n",
        "with CONFIG_PATH.open() as fh:\n",
        "    smollm_config = yaml.safe_load(fh)\n",
        "\n",
        "summary = {\n",
        "    \"hidden_size\": smollm_config[\"model\"][\"model_config\"][\"hidden_size\"],\n",
        "    \"layers\": smollm_config[\"model\"][\"model_config\"][\"num_hidden_layers\"],\n",
        "    \"heads\": smollm_config[\"model\"][\"model_config\"][\"num_attention_heads\"],\n",
        "    \"intermediate_size\": smollm_config[\"model\"][\"model_config\"][\"intermediate_size\"],\n",
        "    \"seq_length\": smollm_config[\"tokens\"][\"sequence_length\"],\n",
        "    \"lr\": smollm_config[\"optimizer\"][\"learning_rate_scheduler\"][\"learning_rate\"],\n",
        "    \"warmup_steps\": smollm_config[\"optimizer\"][\"learning_rate_scheduler\"][\"lr_warmup_steps\"],\n",
        "    \"grad_accum\": smollm_config[\"tokens\"][\"batch_accumulation_per_replica\"],\n",
        "    \"micro_batch\": smollm_config[\"tokens\"][\"micro_batch_size\"],\n",
        "}\n",
        "\n",
        "print(\"Loaded config from\", CONFIG_PATH)\n",
        "print(json.dumps(summary, indent=2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Upload and Inspect `input.txt`\n",
        "\n",
        "Upload the provided corpus from your local machine. After the upload completes, the file will be available in the Colab working directory.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "assert \"input.txt\" in uploaded, \"Please upload input.txt before continuing.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "corpus_path = Path(\"input.txt\")\n",
        "print(f\"input.txt size: {corpus_path.stat().st_size / 1024:.1f} KB\")\n",
        "print(\"--- sample ---\")\n",
        "print(corpus_path.read_text(encoding=\"utf-8\")[:500])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build Tokenized Dataset\n",
        "\n",
        "We tokenize with the official `HuggingFaceTB/cosmo2-tokenizer`, chunk the text into 2,048-token sequences, and create train/eval splits. Gradient accumulation is used to respect the original micro-batch settings while fitting in Colab memory.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from itertools import chain\n",
        "\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "raw_dataset = load_dataset(\"text\", data_files={\"train\": str(corpus_path)})\n",
        "stage0 = smollm_config[\"data_stages\"][0]\n",
        "seed = stage0.get(\"seed\")\n",
        "if seed is None:\n",
        "    seed = stage0.get(\"data\", {}).get(\"seed\")\n",
        "if seed is None:\n",
        "    seed = smollm_config.get(\"general\", {}).get(\"seed\", 42)\n",
        "split_dataset = raw_dataset[\"train\"].train_test_split(test_size=0.1, seed=seed)\n",
        "print(\"Using seed:\", seed)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    smollm_config[\"tokenizer\"][\"tokenizer_name_or_path\"],\n",
        "    revision=smollm_config[\"tokenizer\"].get(\"tokenizer_revision\"),\n",
        ")\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({\"pad_token\": \"<|pad|>\"})\n",
        "    model_pad_added = True\n",
        "else:\n",
        "    model_pad_added = False\n",
        "\n",
        "block_size = smollm_config[\"tokens\"][\"sequence_length\"]\n",
        "\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"])  # returns input_ids and attention_mask\n",
        "\n",
        "\n",
        "tokenized_dataset = split_dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    num_proc=2,\n",
        "    remove_columns=[\"text\"],\n",
        ")\n",
        "\n",
        "pad_token_id = tokenizer.pad_token_id\n",
        "attention_pad_token_id = 0\n",
        "\n",
        "\n",
        "def group_texts(examples):\n",
        "    concatenated = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
        "    total_length = len(concatenated[\"input_ids\"])\n",
        "    if total_length < block_size:\n",
        "        pad_length = block_size - total_length\n",
        "        padded = {}\n",
        "        for k, sequence in concatenated.items():\n",
        "            pad_token = attention_pad_token_id if k == \"attention_mask\" else pad_token_id\n",
        "            padded[k] = [sequence + [pad_token] * pad_length]\n",
        "        padded[\"labels\"] = padded[\"input_ids\"].copy()\n",
        "        return padded\n",
        "\n",
        "    total_length = (total_length // block_size) * block_size\n",
        "    result = {\n",
        "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
        "        for k, t in concatenated.items()\n",
        "    }\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return result\n",
        "\n",
        "lm_datasets = tokenized_dataset.map(group_texts, batched=True, num_proc=2)\n",
        "print(lm_datasets)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stage 1: Train for 5,000 Steps\n",
        "\n",
        "We mirror the YAML hyperparameters (optimizer, scheduler, gradient clipping, etc.) and adapt them to Colab with gradient accumulation. A callback prints generated text every 500 steps. Training can take ~1-2 hours on a T4 and is much faster on an A100.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoConfig,\n",
        "    AutoModelForCausalLM,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    TrainerCallback,\n",
        ")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    major_capability, _ = torch.cuda.get_device_capability()\n",
        "    bf16_enabled = major_capability >= 8\n",
        "    fp16_enabled = False  # disable AMP FP16 to avoid GradScaler issues on older GPUs\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    if not bf16_enabled and major_capability < 8:\n",
        "        print(\"CUDA device lacks native bfloat16; using full precision training to avoid FP16 GradScaler issues.\")\n",
        "else:\n",
        "    bf16_enabled = False\n",
        "    fp16_enabled = False\n",
        "\n",
        "if bf16_enabled:\n",
        "    target_dtype = torch.bfloat16\n",
        "else:\n",
        "    target_dtype = torch.float32\n",
        "print(f\"Loading model weights in dtype: {target_dtype}\")\n",
        "\n",
        "model_name = \"HuggingFaceTB/SmolLM2-135M\"\n",
        "flash_attn_available = False\n",
        "try:\n",
        "    import importlib\n",
        "\n",
        "    flash_attn_available = importlib.util.find_spec(\"flash_attn\") is not None\n",
        "except Exception:\n",
        "    flash_attn_available = False\n",
        "\n",
        "attn_impl = \"flash_attention_2\" if (torch.cuda.is_available() and flash_attn_available) else None\n",
        "if attn_impl is None and torch.cuda.is_available():\n",
        "    print(\"FlashAttention 2 not available; falling back to default attention.\")\n",
        "\n",
        "config = AutoConfig.from_pretrained(model_name)\n",
        "config.vocab_size = len(tokenizer)\n",
        "config.pad_token_id = tokenizer.pad_token_id\n",
        "if tokenizer.bos_token_id is not None:\n",
        "    config.bos_token_id = tokenizer.bos_token_id\n",
        "if tokenizer.eos_token_id is not None:\n",
        "    config.eos_token_id = tokenizer.eos_token_id\n",
        "\n",
        "model = AutoModelForCausalLM.from_config(config)\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "print(\"Initialized SmolLM2-135M from config with randomly-initialized weights.\")\n",
        "if model_pad_added:\n",
        "    print(f\"Extended vocab size to {len(tokenizer)} to include the new pad token.\")\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "if model_pad_added:\n",
        "    print(\"Model embeddings resized to account for newly added pad token.\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "train_dataset = lm_datasets[\"train\"]\n",
        "eval_dataset = lm_datasets[\"test\"]\n",
        "\n",
        "output_dir = Path(\"smollm2_135m_runs\")\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "stage1_dir = output_dir / \"stage1_5000_steps\"\n",
        "stage1_dir.mkdir(exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SampleGenerationCallback(TrainerCallback):\n",
        "    def __init__(self, tokenizer, sample_prompt, sample_interval=500, max_new_tokens=200):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.sample_prompt = sample_prompt\n",
        "        self.sample_interval = sample_interval\n",
        "        self.max_new_tokens = max_new_tokens\n",
        "        prompt = tokenizer(sample_prompt, return_tensors=\"pt\", padding=True)\n",
        "        self.input_ids = prompt[\"input_ids\"]\n",
        "        self.attention_mask = prompt.get(\"attention_mask\")\n",
        "\n",
        "    def on_step_end(self, args, state, control, **kwargs):\n",
        "        if state.global_step == 0:\n",
        "            return control\n",
        "        if state.global_step % self.sample_interval != 0:\n",
        "            return control\n",
        "\n",
        "        model = kwargs[\"model\"]\n",
        "        was_training = model.training\n",
        "        model.eval()\n",
        "        try:\n",
        "            generate_kwargs = {\n",
        "                \"max_new_tokens\": self.max_new_tokens,\n",
        "                \"do_sample\": True,\n",
        "                \"temperature\": 0.8,\n",
        "                \"top_p\": 0.95,\n",
        "                \"eos_token_id\": self.tokenizer.eos_token_id,\n",
        "            }\n",
        "            if self.attention_mask is not None:\n",
        "                generate_kwargs[\"attention_mask\"] = self.attention_mask.to(model.device)\n",
        "            generated = model.generate(\n",
        "                self.input_ids.to(model.device),\n",
        "                **generate_kwargs,\n",
        "            )\n",
        "            decoded = self.tokenizer.decode(generated[0], skip_special_tokens=True)\n",
        "            print(f\"\\n===== Sample @ step {state.global_step} =====\\n{decoded}\\n==============================\\n\")\n",
        "        finally:\n",
        "            if was_training:\n",
        "                model.train()\n",
        "\n",
        "        return control\n",
        "\n",
        "\n",
        "sample_prompt = corpus_path.read_text(encoding=\"utf-8\")[:400]\n",
        "sample_callback = SampleGenerationCallback(tokenizer, sample_prompt)\n",
        "callbacks = [sample_callback]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "grad_accum_target = smollm_config[\"tokens\"][\"batch_accumulation_per_replica\"]\n",
        "micro_batch_target = smollm_config[\"tokens\"][\"micro_batch_size\"]\n",
        "per_device_train_batch_size = 1  # fits T4\n",
        "per_device_eval_batch_size = 1\n",
        "\n",
        "if micro_batch_target % per_device_train_batch_size == 0:\n",
        "    gradient_accumulation_steps = micro_batch_target // per_device_train_batch_size\n",
        "else:\n",
        "    gradient_accumulation_steps = math.ceil(micro_batch_target / per_device_train_batch_size)\n",
        "\n",
        "optim_name = \"adamw_torch_fused\" if (torch.cuda.is_available() and smollm_config[\"optimizer\"][\"optimizer_factory\"].get(\"torch_adam_is_fused\", False)) else \"adamw_torch\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=str(stage1_dir),\n",
        "    overwrite_output_dir=True,\n",
        "    max_steps=5000,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    per_device_eval_batch_size=per_device_eval_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=500,\n",
        "    logging_steps=50,\n",
        "    logging_first_step=True,\n",
        "    learning_rate=smollm_config[\"optimizer\"][\"learning_rate_scheduler\"][\"learning_rate\"],\n",
        "    warmup_steps=smollm_config[\"optimizer\"][\"learning_rate_scheduler\"][\"lr_warmup_steps\"],\n",
        "    weight_decay=smollm_config[\"optimizer\"][\"weight_decay\"],\n",
        "    max_grad_norm=smollm_config[\"optimizer\"][\"clip_grad\"],\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    optim=optim_name,\n",
        "    adam_beta1=smollm_config[\"optimizer\"][\"optimizer_factory\"][\"adam_beta1\"],\n",
        "    adam_beta2=smollm_config[\"optimizer\"][\"optimizer_factory\"][\"adam_beta2\"],\n",
        "    adam_epsilon=smollm_config[\"optimizer\"][\"optimizer_factory\"][\"adam_eps\"],\n",
        "    report_to=[\"tensorboard\"],\n",
        "    bf16=bf16_enabled,\n",
        "    fp16=fp16_enabled and not bf16_enabled,\n",
        "    gradient_checkpointing=smollm_config[\"parallelism\"].get(\"recompute_layer\", False),\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=5000,\n",
        "    save_total_limit=2,\n",
        "    logging_dir=str(stage1_dir / \"logs\"),\n",
        "    push_to_hub=False,\n",
        ")\n",
        "\n",
        "print(training_args)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    data_collator=data_collator,\n",
        "    callbacks=callbacks,\n",
        "    preprocess_logits_for_metrics=None,\n",
        ")\n",
        "\n",
        "train_result = trainer.train()\n",
        "trainer.save_model(str(stage1_dir / \"checkpoint-final\"))\n",
        "metrics = train_result.metrics\n",
        "trainer.log_metrics(\"train\", metrics)\n",
        "trainer.save_metrics(\"train\", metrics)\n",
        "trainer.save_state()\n",
        "\n",
        "final_eval = trainer.evaluate()\n",
        "trainer.log_metrics(\"eval\", final_eval)\n",
        "trainer.save_metrics(\"eval\", final_eval)\n",
        "\n",
        "print(\"Stage 1 completed. Final checkpoint stored under:\", stage1_dir)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sync trainer state into the final checkpoint directory for easy resumption\n",
        "final_checkpoint_dir = stage1_dir / \"checkpoint-final\"\n",
        "final_checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "trainer.state.save_to_json(str(final_checkpoint_dir / \"trainer_state.json\"))\n",
        "try:\n",
        "    trainer.save_optimizer_and_scheduler(str(final_checkpoint_dir))\n",
        "except Exception as err:\n",
        "    print(f\"Warning: could not persist optimizer/scheduler to {final_checkpoint_dir}: {err}\")\n",
        "else:\n",
        "    print(f\"Saved trainer state, optimizer, and scheduler to {final_checkpoint_dir}.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stage 2: Resume for 50 Additional Steps\n",
        "\n",
        "After the initial 5,000 steps, reload the checkpoint and continue for 50 more steps (total 5,050). The same evaluation cadence and sampling callback are reused.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "stage1_model_dir = stage1_dir / \"checkpoint-final\"\n",
        "if not stage1_model_dir.exists():\n",
        "    stage1_model_dir = stage1_dir\n",
        "\n",
        "resume_checkpoint = stage1_dir if (stage1_dir / \"trainer_state.json\").exists() else stage1_model_dir\n",
        "\n",
        "print(f\"Stage 2 will load weights from: {stage1_model_dir}\")\n",
        "print(f\"Stage 2 will resume optimizer state from: {resume_checkpoint}\")\n",
        "\n",
        "stage2_dir = output_dir / \"stage2_5050_steps\"\n",
        "stage2_dir.mkdir(exist_ok=True)\n",
        "\n",
        "model_stage2 = AutoModelForCausalLM.from_pretrained(\n",
        "    stage1_model_dir,\n",
        "    torch_dtype=target_dtype,\n",
        "    attn_implementation=attn_impl,\n",
        "    device_map=\"auto\" if torch.cuda.device_count() > 1 else None,\n",
        ")\n",
        "if model_pad_added:\n",
        "    model_stage2.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "training_args_stage2 = TrainingArguments(\n",
        "    output_dir=str(stage2_dir),\n",
        "    overwrite_output_dir=True,\n",
        "    max_steps=5050,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    per_device_eval_batch_size=per_device_eval_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=500,\n",
        "    logging_steps=50,\n",
        "    logging_first_step=True,\n",
        "    learning_rate=smollm_config[\"optimizer\"][\"learning_rate_scheduler\"][\"learning_rate\"],\n",
        "    warmup_steps=smollm_config[\"optimizer\"][\"learning_rate_scheduler\"][\"lr_warmup_steps\"],\n",
        "    weight_decay=smollm_config[\"optimizer\"][\"weight_decay\"],\n",
        "    max_grad_norm=smollm_config[\"optimizer\"][\"clip_grad\"],\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    optim=optim_name,\n",
        "    adam_beta1=smollm_config[\"optimizer\"][\"optimizer_factory\"][\"adam_beta1\"],\n",
        "    adam_beta2=smollm_config[\"optimizer\"][\"optimizer_factory\"][\"adam_beta2\"],\n",
        "    adam_epsilon=smollm_config[\"optimizer\"][\"optimizer_factory\"][\"adam_eps\"],\n",
        "    report_to=[\"tensorboard\"],\n",
        "    bf16=bf16_enabled,\n",
        "    fp16=fp16_enabled and not bf16_enabled,\n",
        "    gradient_checkpointing=smollm_config[\"parallelism\"].get(\"recompute_layer\", False),\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=5050,\n",
        "    save_total_limit=2,\n",
        "    logging_dir=str(stage2_dir / \"logs\"),\n",
        "    push_to_hub=False,\n",
        ")\n",
        "\n",
        "trainer_stage2 = Trainer(\n",
        "    model=model_stage2,\n",
        "    args=training_args_stage2,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    data_collator=data_collator,\n",
        "    callbacks=callbacks,\n",
        "    preprocess_logits_for_metrics=None,\n",
        ")\n",
        "\n",
        "train_result_stage2 = trainer_stage2.train(resume_from_checkpoint=str(resume_checkpoint))\n",
        "trainer_stage2.save_model(str(stage2_dir / \"checkpoint-final\"))\n",
        "metrics_stage2 = train_result_stage2.metrics\n",
        "trainer_stage2.log_metrics(\"train\", metrics_stage2)\n",
        "trainer_stage2.save_metrics(\"train\", metrics_stage2)\n",
        "trainer_stage2.save_state()\n",
        "\n",
        "eval_stage2 = trainer_stage2.evaluate()\n",
        "trainer_stage2.log_metrics(\"eval\", eval_stage2)\n",
        "trainer_stage2.save_metrics(\"eval\", eval_stage2)\n",
        "\n",
        "print(\"Stage 2 completed. Final checkpoint stored under:\", stage2_dir)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps & Notes\n",
        "\n",
        "- Use `tensorboard --logdir smollm2_135m_runs/stage1_5000_steps/logs` (or stage2) to inspect metrics.\n",
        "- Download checkpoints via the Colab file browser or `from google.colab import files; files.download(...)`.\n",
        "- To push to the Hugging Face Hub, call `trainer.push_to_hub()` or manually upload the stage directories.\n",
        "- If you need shorter context windows (e.g., due to limited RAM), lower `block_size` before tokenization and adjust `max_position_embeddings` in the model config accordingly.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
